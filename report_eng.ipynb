{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data-Driven Analysis of Hacken Lee's Setlist for **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "### 1.1 Project Goals\n",
    "Analyze the track selection from three Hacken Lee's concerts that share the same theme. \n",
    "\n",
    "\n",
    "\n",
    "Specifically, this project explores:\n",
    "- *How are hit songs and lesser-known sidetracks balanced across the setlists?*\n",
    "- *Which albums are most favoured?*\n",
    "- *How do the setlists vary across different concert locations?*\n",
    "\n",
    "### 1.2 Why Hacken Lee?\n",
    "First of all, I like him. \n",
    "\n",
    "Secondly, Hacken Lee is a prominent Cantopop singer known for his rich discography: he is one of the few singers whose career spans from the 1980s to the 2020s, maintaining both popularity and a steady output of albums. He is also among the rare artists who received music awards in Hong Kong across every decade from the 1980s to the 2010s. \n",
    "\n",
    "For established singers like Hacken, with a vast pool of possible songs to perform, crafting a setlist often means striking a balance between hit songs and lesser-known sidetracks. This characteristic makes his concerts especially interesting for setlist analysis.\n",
    "\n",
    "\n",
    "\n",
    "### 1.3 Why These Three Concerts?\n",
    "\n",
    "In May 2023, Hacken Lee collaborated once again with the Hong Kong Philharmonic Orchestra for a concert series at the Hong Kong Coliseum. Following that, he brought the concert on tour to mainland China in 2024 with the Guangzhou Symphony Orchestra. Earlier this year (2025), he teamed up with the Macao Orchestra for the third time to present the highly acclaimed concert of the same theme in Macau.\n",
    "\n",
    "Although the concert title varied slightly at each location—**Hacken Lee x HK Phil 2023** (弦續 李克勤·港樂演唱會) in Hong Kong, **弦续 李克勤巡回演唱会** in mainland China, and **Hacken Lee Symphonic Live in Londoner with Macao Orchestra** (李克勤·我們的交響樂) in Macau—the central themes remained consistent: *continuation* and *collaboration* \n",
    "\n",
    "- *Continuation* as  by the title (also the theme song) 「弦續」 (a homophone of 「延續」 in Cantonese), symbolizing the continuation of music\n",
    "-  *Collaboration* is embodied in the fusion with live orchestras\n",
    "\n",
    "The three setlists chosen are from:\n",
    "- **2023/05/20 @ Hong Kong**  (data sourced from *HackenZone*@FB)\n",
    "- **2024/11/30 @ Guangzhou**  (firsthand experience)\n",
    "- **2025/02/09 @ Macau**         (also firsthand)\n",
    "\n",
    "These represent the finale concerts for the Hong Kong leg, the mainland China tour, and the Macau shows respectively. *(Although the Macau concert wasn't actually the final one—as Hacken will be returned to perform there later this year—I chose it under that assumption while working on this project.)*\n",
    "\n",
    "The encore section of a live concert is especially critical in leaving a lasting impression. The finale concert usually has the most electrifying atmosphere, and performers often sing more songs during the encore. For example, Hacken performed **14 encore songs** at the finale of his *30th Anniversary Concert* in 2017.\n",
    "\n",
    "Another reason for choosing these three concerts is that Cantonese is the dominant spoken language in all three cities. As Hacken has released albums in both Cantonese and Mandarin, he often adapts his setlists based on the audience’s language. In non-Cantonese-speaking areas, some tracks are swapped out. The consistency in language across these three locations allows for a more cohesive and meaningful comparison of setlist design.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Retrieving information from Spotify\n",
    "### 2.1 Setup & Authorization\n",
    "There is a Python library for the Spotify Web API called [Spotipy](https://spotipy.readthedocs.io/en/2.25.1/), which can be used for retrieving music data provided by the Spotify platform. For <b>authorization</b>, I followed the step on <em>Spotipy</em> documentation page and set environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spotipy\n",
    "import time\n",
    "import requests\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "\n",
    "#spo = spotipy.Spotify(client_credentials_manager=SpotifyClientCredentials())\n",
    "path = \"dataset/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Retrieving overall album data\n",
    "The first step is to grab the whole album list for the given artist `id` using `artist_albums()`.\n",
    "\n",
    "NOTE1: The parameter `include_groups` have four valid values: `album`, `single`, `appears_on` and `compilation`. For the purpose of this project, there is no need to retrieve information `appears_on` type since it is mostly consisted of Spotify's self-made set of tracks, rather than those officially released by record companies.\n",
    "\n",
    "NOTE2: `artist_albums()` has a maximum number (**50**) of items to return. The while loop is needed if the artist has over 50 albums.\n",
    "\n",
    "`pandas` library is then used to turn the results into a dataframe object, drop unnecessary columns, and save the file locally.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "albums = []\n",
    "hacken = spo.artist(\"https://open.spotify.com/artist/3PV11RNUoGfX9tMN2wVljB\")\n",
    "\n",
    "album = spo.artist_albums(hacken['id'], include_groups='album',limit=50)\n",
    "single = spo.artist_albums(hacken['id'], include_groups='single',limit=50)\n",
    "compilation = spo.artist_albums(hacken['id'], include_groups='compilation',limit=50)\n",
    "albums.extend(album['items'])\n",
    "albums.extend(single['items'])\n",
    "albums.extend(compilation['items'])\n",
    "\n",
    "while album['next']:\n",
    "    album = spo.next(album)\n",
    "    albums.extend(album['items'])\n",
    "while single['next']:\n",
    "    single = spo.next(single)\n",
    "    albums.extend(single['items'])\n",
    "while compilation['next']:\n",
    "    compilation = spo.next(compilation)\n",
    "    albums.extend(compilation['items'])\n",
    "\n",
    "df = pd.DataFrame(data=albums,columns=['name','id','release_date','album_type'])\n",
    "df.to_excel(path+\"Hacken_Lee_Albums_Full.xlsx\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will yield a overall album dataset of **4** columns(`name`, `id`, `release_date`, `album_type`)\n",
    "![](img/ex0.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_overall = pd.read_excel(path+\"Hacken_Lee_Albums_Full.xlsx\")\n",
    "ex_overall.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Retrieving track data for each album\n",
    "For each album, `album_tracks()` is used to get every track information inside that album. Only the `id` and `name` of each track data are necessary for the next step, once again `pandas` is used.\n",
    "\n",
    "NOTE: `time.sleep()` is extremely needed in this step, otherwise the process would be cutoff easily.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(albums['id'])):\n",
    "    print(albums['name'][i],\" started\")\n",
    "   \n",
    "    results = spo.album_tracks(albums['id'][i])\n",
    "    tracks = results['items']\n",
    "    while results['next']:\n",
    "        results = spo.next(results)\n",
    "        tracks.extend(results['items'])\n",
    "        \n",
    "    tracks_df = pd.DataFrame(data=tracks,columns=['name','id'])\n",
    "    print(albums['name'][i],\" dataframe generated\")\n",
    "    tracks_df[\"album_id\"] = albums['id'][i]\n",
    "    tracks_df.to_excel(path+str(albums['name'][i])+\".xlsx\",index=False)\n",
    "    print(str(albums['name'][i]),\" finished\")\n",
    "    time.sleep(30) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, a separate dataset is created for each album, contianing **3** columns:\n",
    "- `name` (track title)\n",
    "- `id` (track unique identifier)\n",
    "- `album_id` (parent album id)\n",
    "\n",
    "![](img/ex1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_album = pd.read_excel(path+\"李克勤慶祝成立30週年演唱會.xlsx\")\n",
    "ex_album.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Retrieving audio features for each track\n",
    "Here comes the main part of data extraction. One of the most valuable asset of Spotify's database is the audio features of each track. The function `audio_features()` supports a list of `id` as parameter, which simplifies the process -- instead of retrieving audio features of a single track every time, passing as a list reduces the number of calling `audio_features()` and avoiding possible timeout. \n",
    "\n",
    "`Pandas` is once again used to drop unnecessary columns and save the file.\n",
    "\n",
    "NOTE1: please handle the parameter for `time.sleep()` in this step carefully\n",
    "\n",
    "NOTE2: the information provided by `audio_features()` doesn't include the name of the album and the name of track, to add these:\n",
    "- `features_df['album_name'] = albums['name'][i]` *to copy cell value at the ith row, name column from the overall album dataset* (`album_name`)\n",
    "- `features_df.loc[:,'track_name'] = album['name']` *to copy the whole name column from the current album dataset* (`track_name`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(albums['id'])):\n",
    " \n",
    "    if (i>20 and i%20==0):\n",
    "        time.sleep(5)\n",
    "    print(albums['name'][i]+\" started\")\n",
    "   \n",
    "    album = pd.read_excel(path+albums['name'][i]+'.xlsx') \n",
    " \n",
    "    track_ids = []\n",
    "    for j in range(len(album['id'])):\n",
    "        track_ids.append(album['id'][j])\n",
    "    time.sleep(1)\n",
    "    print(\"Track ID concatenated\")\n",
    "    results = spo.audio_features(track_ids)\n",
    "    \n",
    "    features_df = pd.DataFrame(data=results,columns=results[0].keys())\n",
    "    print(\"DataFrame Generated\")\n",
    "    time.sleep(1)\n",
    "    features_df = features_df.drop(columns=['type','uri','track_href','analysis_url'])\n",
    " \n",
    "    features_df['album_name'] = albums['name'][i]\n",
    "    features_df.loc[:,'track_name'] = album['name']\n",
    "    features_df.to_excel(path+albums['name'][i]+'_features.xlsx',index=False)\n",
    "    \n",
    "    print(albums['name'][i]+\" finished\")\n",
    "    print('time to sleep')\n",
    "    time.sleep(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the last step, this will generate a  dataframe that have **14** columns for every album. Below is an example\n",
    "![](img/ex2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_album_feature = pd.read_excel(path+\"李克勤慶祝成立30週年演唱會_features.xlsx\")\n",
    "ex_album_feature.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Concatenating the feature datasets as one dataset\n",
    "For the purpose of searching and extracting one specific track information, concatenating the feature datasets into one overall dataset would be extremely useful in terms of saving time. \n",
    "\n",
    "NOTE: For readability's sake, this line `album = album.iloc[:, [15, 11, 14, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13]] ` is used to reorder the columns, so that `track_name`, `id`, `album_name` would be the first three columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "album_features = []\n",
    "total_albums_features = []  \n",
    "for i in range(len(albums['id'])):\n",
    "    album = pd.read_excel(path + albums['name'][i] + '_features.xlsx')\n",
    "    album = album.iloc[:, [15, 11, 14, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13]] \n",
    "    albums.append(album)\n",
    "\n",
    "album_features_total = pd.concat(total_albums_features)\n",
    "album_features_total.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Retrieving the PlayCount data using Public API\n",
    "**Disclaimer**: This project is to be used for **education purpose** only.\n",
    "\n",
    "Let's back to this project's first research question: <em>How are hit songs and lesser-known sidetracks balanced across the setlists?</em>. To answer this, we need to define what are hit songs and what are sidetracks. Every fan may give a different opinion about this. Thus, an objective measure is needed. \n",
    "\n",
    "#### 2.6.1 Popularity data from Spotify and why not applicable\n",
    "Spotify's API provides the popularity data of a track object, which <em>\"is calculated by algorithm and is based, in the most part, on the total number of plays the track has had and how recent those plays are\"</em>. In some cases it may reflect how popular the track is, but its nature in which duplicate tracks are rated independently determined that it's not suitable for this project.\n",
    "\n",
    "Why? It's not unusual that record companies release compilation albums every few years and listeners may tend to listen one compilation album rather than search for the particular album when one song is released in. Therefore, it's not really a proper choice as one track's popularity data would be dispersed into different version.\n",
    "\n",
    "#### 2.6.2 PlayCount data using Public API\n",
    "Spotify also have the play counts for each track, although not accessible using its official API. After doing some research, I found out that for the same track, Spotify adds up every version's play counts and treat it the same for every version -- which is exactly what I need. \n",
    "\n",
    "A sincere thank to the author of [this github project](https://github.com/entriphy/sp-playcount-librespot), who provides a public api to retrieve the play count of all tracks in a Spotify album.\n",
    "\n",
    "`requests` library is used to handle the response returned, `pandas` is then used to merge the results to previous features dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hacken_albums = pd.read_excel(path+\"Hacken_Lee_Albums_Full.xlsx\")\n",
    "\n",
    "base_url = \"https://api.t4ils.dev/albumPlayCount\"\n",
    "for i in range(len(hacken_albums['id'])):\n",
    "    if (i>61 and i%10==0):\n",
    "        print(\"it's time to sleep\")\n",
    "        time.sleep(5)\n",
    "    album_id = hacken_albums['id'][i]  \n",
    "\n",
    "  \n",
    "    url = f\"{base_url}?albumid={album_id}\"\n",
    "    response = requests.get(url)\n",
    "    print(hacken_albums['name'][i],\" started\")\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "    \n",
    "        if data.get('success'):\n",
    "            play_count_data = []\n",
    "            album_data = data.get('data',{})\n",
    "            for disc in album_data.get('discs', []):\n",
    "                for track in disc.get('tracks', []):\n",
    "                    track_id = track.get('uri').split(\":\")[-1] \n",
    "                    play_count = track.get('playcount')\n",
    "                    play_count_data.append({'id': track_id, 'play_count': play_count})\n",
    "            \n",
    "         \n",
    "            play_count_df = pd.DataFrame(play_count_data)\n",
    "            print(play_count_df.head())\n",
    "          \n",
    "            existing_data = pd.read_excel(path+hacken_albums['name'][i]+\"_features.xlsx\")\n",
    "            updated_data = existing_data.merge(play_count_df, on='id', how='left')\n",
    "        \n",
    "            updated_data.to_excel(path+hacken_albums['name'][i]+\"_features.xlsx\", index=False)\n",
    "            print(\"Play count data successfully merged into the Excel sheet.\")\n",
    "        \n",
    "        else:\n",
    "            print(\"API response indicates failure.\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data. HTTP Status Code: {response.status_code}\")\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6.3 Update the overall feature dataset\n",
    "After grabbing the play count data for each album, the overall dataset needs to be updated as follows. \n",
    "\n",
    "NOTE: The order of albums in the overall feature dataset follows the same order as in the overall album dataset, which simplifies the process -- just a nested for loop will do the trick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playcount_list = []\n",
    "\n",
    "for i in range(len(hacken_albums['id'])):\n",
    "    album_name = hacken_albums.loc[i, \"name\"]\n",
    "    print(f\"{album_name} started\")\n",
    "\n",
    "    album = pd.read_excel(path + album_name + \"_features.xlsx\")\n",
    "  \n",
    "    for j in range(len(album['id'])):\n",
    "        playcount_list.append(album.loc[j, 'play_count'])\n",
    "\n",
    "hacken_features = pd.read_excel(path + \"Hacken_Lee_Albums_Features_Full.xlsx\")\n",
    "hacken_features['play_count'] = playcount_list\n",
    "\n",
    "hacken_features.to_excel(path+\"Hacken_Lee_Albums_Features_Updated.xlsx\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Till now, everything needed from Spotify has been retrieved and saved in several datasets. Let's proceed to extract setlist data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extracting Setlist Data\n",
    "NOTE: In this part, only two overall datasets are needed:\n",
    "- `hacken_albums`: the overall album list which contains every album's `name`, `id`, `release_date` and `album_type`\n",
    "- `hacken_features`: the overall feature list which contains every track's audio features data, album_name, track_name, and play_count\n",
    "\n",
    "### 3.1 Searching for specific track data\n",
    "The first step is obviously to make a list of names of the tracks that Hacken performed in those three concerts respectively. Using `pandas`'s function `.isin()` can do a quick and shallow filter based on the tracks' name. But a problem emerges as follows: how to deal with duplicates? Pick my favourite song as an example, here is the result for `hacken_features[hacken_features['track_name'].str.contains(\"好戲之人\")][[\"track_name\",\"album_name\",\"play_count\"]]`.\n",
    "![](img/ex3.png)\n",
    "\n",
    "It's clearly that one single track would have several versions (studio album, compilation, or live). In order to answer the second research question: *Which albums are most favoured?*, only one version is needed, that is, the first album that the track is released in. \n",
    "\n",
    "**NOTE**:\n",
    "1. Since the feature dataset doesn't contain information of release date, a merge operation is required. The logic of this merge step is to add the `release_date` cell value based on the match for album name. As the representative column of album name have different names (`album_name` in feature dataset and `name` in album dataset), an *inner join* is the only method that works.\n",
    "\n",
    "2. The type of `release_date` is string, it needs to be reformated to datetime object first. \n",
    "\n",
    "Below is an example code for extracting the data for setlist in Macau concert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_list_macau = [\n",
    "    \"希望\", \"一個都不能少\", \"破曉時份\", \"只想你會意\", \"舊歡如夢\", \n",
    "    \"護花使者\", \"藍月亮\", \"C3PO\", \"回首\", \"一生不變\", \n",
    "    \"Victory\", \"天水、圍城\", \"刻不容緩\", \"最愛\", \"沒有你贏了世界又如何\",\"告別校園時\",\n",
    "    \"再見演奏廳\",\"深深深\",\"紅日\",\"夏日之神話\",\"月半小夜曲\",\"我不會唱歌\",\"高妹\",\"合久必分\",\"飛花\"\n",
    "]\n",
    "\n",
    "\n",
    "filtered_df = hacken_features[hacken_features['track_name'].isin(track_list_macau)]\n",
    "\n",
    "\n",
    "merged_filtered_df = pd.merge(\n",
    "    filtered_df, \n",
    "    hacken_albums[['name', 'release_date']], \n",
    "    left_on='album_name', \n",
    "    right_on='name', \n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "\n",
    "merged_filtered_df['release_date'] = pd.to_datetime(merged_filtered_df['release_date'])\n",
    "\n",
    "plot_data = merged_filtered_df.loc[merged_filtered_df.groupby('track_name')['release_date'].idxmin()]\n",
    "\n",
    "plot_data['release_year'] = plot_data['release_date'].dt.year\n",
    "\n",
    "plot_data = plot_data.drop(columns=['release_date',\"name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Manual Adjustment\n",
    "#### 3.2.1 Correction for album name\n",
    "\n",
    "Due to copyright issues, some of Hacken's albums are not available on Spotify. (Particularly those released by Impact Entertainment.) Thus, in the last step of selecting the first release album, what is selected is the earliest album that Spotify has data for. To pursue authenticity, manual correction for albums is necessary in this step.\n",
    "\n",
    "#### 3.2.2 Adding record company information\n",
    "\n",
    "For visualization, I find it would be more intuitive if every track/album is labelled with a color corresponding to the record company.\n",
    "\n",
    "As said in the introduction, Hacken's career spans from the 1980s to 2020s (and is still ongoing), *unavoidably* (I use this word in considering the fact of record industry development in Hong Kong from the 1980s to now) he has collaborated with 5 different record companies:\n",
    "- PolyGram (*1986-1993*)\n",
    "- Star (*1993-1996*)\n",
    "- Music Impact Entertainment (*1996-1998*)\n",
    "- Universal Music Hong Kong (*1999-2016*)\n",
    "- Emperor Entertainment Group (*2016-now*)\n",
    "\n",
    "Although Spotify contains a copyright statement for an album, for some albums the information is missing. In this case, it would be more time-saving to enter the record company value manually as I have prior knowledge (with the help of those physical CDs I owned).\n",
    "\n",
    "#### 3.2.3 Classifying tracks based on position\n",
    "Normally a setlist of a concert is consisted of fixed songs and encore songs. Classifying the tracks into `main` and `encore` types allows me to have a better examine at the first research question.\n",
    "\n",
    "**NOTE**\n",
    "1. Usually the encore part lies the last of a concert, but here the setlist of the Hong Kong one makes the exception -- as Hacken performed different songs every night during the orchestra's intermission time. Hence I feel more appropriate to put those four songs into the category of '*encore songs*'.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hk_setlist = pd.read_excel(path+\"23HongKong.xlsx\")\n",
    "hk_setlist.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualization\n",
    "### 4.1 Quadrant Analysis\n",
    "Similar to a scatter plot, a quadrant chart is used to identify patterns or trends in data using an XY axis. However, what distinguishes a quadrant chart is that the origin of the two axes lies at the middle point, typically the center of the field, thus forming four distinct areas. The key difference between these two types of charts is the direction of values increases -- in quadrant charts, values increase in positive and negative directions as they expand from the center. \n",
    "\n",
    "To address the first question: *How are hit songs and lesser-known sidetracks balanced across the setlists?* The play count data serves as one axis, as it was established in **Section 2.6** as the objective classification metric. A straightforward approach is to use the average play count as a threshold: tracks exceeding this value are classified as \"hit songs\", while other are considered sidetracks. \n",
    "\n",
    "For the other axis, the track's release year is selected. The year range is deliberately fixed from *1985* to *2025* (rather than using the actual earliest and latest years) for two reasons:\n",
    "1. A fixed range facilitates cross-comparison among the three setlists\n",
    "2. The middle point, or the *origin*, corresponds to 2005 -- the year Hacken won *Most popular male signer* for the third time\n",
    "\n",
    "\n",
    "Thus, this step aims to create quadrant charts with:\n",
    "- X-axis: Release year (from *1985* to *2025*)\n",
    "- Y-axis: Log-transformed play count (from *8* to *18*)\n",
    "\n",
    "**Key Notes:**\n",
    "1. Log transformation is applied to play counts due to their extreme range (e.g., a ~1 million difference between the top two tracks). Without this, most tracks would cluster near the bottom.\n",
    "2. Color-coding track spots by their record company (via the `record_company` column) may reveal *label-specific* patterns, also color-coding track text by the type (via the `rundown` column) mentioned in **Section 3.2.3** may reveal *position-specific* patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from adjustText import adjust_text\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "\n",
    "matplotlib.rcParams['font.family'] = ['Heiti TC']\n",
    "\n",
    "no_duplicate_df = pd.read_excel(path+\"23HongKong.xlsx\")\n",
    "\n",
    "\n",
    "no_duplicate_df['log_play_count'] = np.log1p(no_duplicate_df['play_count'])\n",
    "\n",
    "color_id = {\n",
    "     '寶麗金': '#84594d',  \n",
    "    '星光': '#a9d8e4',  \n",
    "    '藝能': '#c6c6c6',  \n",
    "    '環球': '#a7dc91',  \n",
    "    '英皇': '#3975b1'   \n",
    "}\n",
    "tran_Name = {\n",
    "    '寶麗金': 'PolyGram', \n",
    "    '星光': 'Star',  \n",
    "    '藝能': 'Music Impact Entertainment',\n",
    "    '環球': 'Universal', \n",
    "    '英皇': 'EEG'  \n",
    "}\n",
    "\n",
    "\n",
    "no_duplicate_df['RecordCompanyEN'] = no_duplicate_df['record_company'].map(tran_Name)\n",
    "no_duplicate_df['color'] = no_duplicate_df['record_company'].map(color_id)\n",
    "rundown_color_map = {'main': 'black', 'encore': '#c9a1ed'}\n",
    "no_duplicate_df['text_color'] = no_duplicate_df['rundown'].map(rundown_color_map)\n",
    "\n",
    "x = no_duplicate_df['release_year']\n",
    "y = no_duplicate_df['log_play_count']\n",
    "\n",
    "\n",
    "x_min, x_max = 1985, 2025\n",
    "y_min, y_max = 8,18\n",
    "x_mid = x_min + (x_max - x_min) / 2\n",
    "y_mid = y_min + (y_max - y_min) / 2\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "for company, group in no_duplicate_df.groupby('RecordCompanyEN'):\n",
    "    ax.scatter(\n",
    "        group['release_year'],\n",
    "        group['log_play_count'],\n",
    "        label=company,\n",
    "        color=group['color'].iloc[0],\n",
    "        edgecolor='black',\n",
    "        s=100\n",
    "        \n",
    "    )\n",
    "\n",
    "texts = []\n",
    "for i, row in no_duplicate_df.iterrows():\n",
    "    texts.append(\n",
    "        ax.text(\n",
    "            row['release_year']+0.3,\n",
    "            row['log_play_count']-0.1,\n",
    "            row['track_name'],\n",
    "            fontsize=12,\n",
    "            ha='left',\n",
    "            color=row['text_color']\n",
    "        )\n",
    "    )\n",
    "adjust_text(texts,arrowprops=dict(arrowstyle='->', color='red'))\n",
    "\n",
    "\n",
    "ax.axvline(x=x_mid, color='#c2d6f3', linestyle='--', linewidth=1)\n",
    "ax.axhline(y=y_mid, color='#c2d6f3', linestyle='--', linewidth=1)\n",
    "ax.set_xlim(1985,2025)\n",
    "ax.set_ylim(8,18)\n",
    "\n",
    "ax.set_title('Hacken Lee X HKPhil Concert 2023 Rundown', fontsize=16)\n",
    "ax.set_xlabel('Release Year', fontsize=12)\n",
    "ax.set_ylabel('Log-Scaled Popularity', fontsize=12)\n",
    "ax.legend(title=\"Record Company\", loc='upper right')\n",
    "plt.text(0.85,\n",
    "         0.77,\n",
    "         \"a watermark by wend1k3\",\n",
    "         transform=plt.gca().transAxes,\n",
    "            ha='center',  \n",
    "            va='center',  \n",
    "            alpha=1,\n",
    "                fontdict=dict(\n",
    "                    fontsize=11,\n",
    "                    color='#ecacbd',\n",
    "                    family=\n",
    "                    \"Heiti TC\",  \n",
    "                    weight=\n",
    "                    'normal', \n",
    "                )  )\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the three quadrant charts corresponding to three setlist.\n",
    "![](img/23HKquadrant.png)\n",
    "![](img/24GZquadrant.png)\n",
    "![](img/25OMquadrant.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Bar plot "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Discussion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
