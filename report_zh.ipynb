{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data-Driven Analysis of Hacken Lee's Setlist for **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 引言\n",
    "\n",
    "### 1.1 项目目标\n",
    "分析李克勤三场同主题歌单的选曲。\n",
    "\n",
    "具体而言，此项目将探讨：\n",
    "- *歌单中热门金曲与沧海遗珠的平衡程度* \n",
    "- *在这次主题巡回中， 哪张专辑是李生心头好*\n",
    "- *粤港澳三地的歌单有何异同*\n",
    "\n",
    "*沧海遗珠\\*: 冷门经典/不被大众熟知但粉丝喜欢的曲目\n",
    "### 1.2 为什么选李克勤作为分析对象？\n",
    "首先，我喜欢他。\n",
    "\n",
    "其次，李克勤是粤语流行乐坛的代表性歌手，拥有丰富的音乐作品：他是少数从1980年代到2020年代持续活跃且保持人气与专辑稳定产出的歌手，更是罕见的在1980年代至2010年代均获得香港乐坛颁奖礼奖项的歌手。\n",
    "\n",
    "对于李生这样作品众多的成熟歌手而言，演唱会选曲往往需要在热门金曲与沧海遗珠之间取得平衡。这一特点是他的演唱会曲目编排尤其值得分析。\n",
    "\n",
    "### 1.3 为什么选粤港澳这三场?\n",
    "在2023年五月，李克勤再度携手香港管弦乐团，在香港红磡体育馆举办「弦續 李克勤·港樂演唱會」个人演唱会。随后于2024年，他与广州交响乐团合作，将「弦續」这一演唱会主题带到中国内地进行巡演。今年初（2025年），他第三度联袂澳门乐团，在澳门呈献了这场广受好评的演唱会。\n",
    "\n",
    "虽然三地在名称上有些许差别——在香港是**弦續 李克勤·港樂演唱會**，在内地是**弦续 李克勤巡回演唱会**，在澳门是**李克勤·我們的交響樂**——但是主题与演出方式是保持一致的:\n",
    "- 主题*弦续* ——粤语音同*延续*，意为音乐的延续\n",
    "- 与各地乐团（港乐、广交、澳门乐团）的合作演出\n",
    "\n",
    "歌单分别选自:\n",
    "- **2023/05/20 香港场** （数据来源：*HackenZone* @FB）\n",
    "- **2024/11/30 广州场** （一手资料）\n",
    "- **2025/02/09 澳门场** （同一手）\n",
    "\n",
    "这三场分别代表了23年香港、24年内地巡回、及23年到25年澳门这三地的收官场次。*(其实澳门场并不是最终的收官因为李生会于今年晚些时候返场——但在做这个项目时我认定是尾场)*\n",
    "\n",
    "演唱会中的安可环节对于塑造持久印象尤为关键。而收官场次又通常拥有最热烈的氛围，歌手也往往会在安可环节加唱更多曲目。例如，李克勤在2017年《李克勤庆祝成立30周年演唱会》尾场时，安可曲目高达14首。\n",
    "\n",
    "基于粤港澳大湾区文化同源的特殊性，本项目选取香港、广州、澳门三地演唱会作为样本更具典型意义。三地不仅共享粤语这一日常主流语言，更在流行文化审美上具有高度共识。李克勤兼具国语和粤语专辑，在非粤语区通常会考虑观众从而替换部分曲目(例如这次巡回将《蓝月亮》改为《爱可以问谁》)。这种文化-语言的双重同质性，能最大排除干扰，使分析更精准聚焦于歌单设计。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 从Spotify获取数据\n",
    "### 2.1 搭建环境与授权认证\n",
    "[Spotipy](https://spotipy.readthedocs.io/en/2.25.1/)是一个通过用Spotify Web API，从Spotify平台获取歌曲数据的Python库。对于授权认证，我按照*Spotipy*文档页上的步骤并设置环境变量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spotipy\n",
    "import time\n",
    "import requests\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "\n",
    "spo = spotipy.Spotify(client_credentials_manager=SpotifyClientCredentials())\n",
    "path = \"dataset/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 获取完整专辑数据\n",
    "首先通过`artist_albums()`方法，根据指定艺人`id`抓取其全部专辑列表。\n",
    "\n",
    "**注意事项**:\n",
    "1. 参数`include_groups`包含四个有效值:\n",
    "    - `album` （正式专辑）\n",
    "    - `single` （单曲）\n",
    "    - `appears_on` (参与作品)\n",
    "    - `compilation` （精选集） \n",
    "    \n",
    "    本项目无需获取`appers_on`类型数据，因该类型多为Spotify自制合辑，而非唱片公司官方发行的作品。\n",
    "2. 该方法单次请求最多返回**50条**数据。若艺人专辑超过50张，需通过while循环实现分页获取。\n",
    "\n",
    "最后使用`pandas`库将结果转换为DataFrame对象，剔除冗余列后保存文件至本地。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "albums = []\n",
    "hacken = spo.artist(\"https://open.spotify.com/artist/3PV11RNUoGfX9tMN2wVljB\")\n",
    "\n",
    "album = spo.artist_albums(hacken['id'], include_groups='album',limit=50)\n",
    "single = spo.artist_albums(hacken['id'], include_groups='single',limit=50)\n",
    "compilation = spo.artist_albums(hacken['id'], include_groups='compilation',limit=50)\n",
    "albums.extend(album['items'])\n",
    "albums.extend(single['items'])\n",
    "albums.extend(compilation['items'])\n",
    "\n",
    "while album['next']:\n",
    "    album = spo.next(album)\n",
    "    albums.extend(album['items'])\n",
    "while single['next']:\n",
    "    single = spo.next(single)\n",
    "    albums.extend(single['items'])\n",
    "while compilation['next']:\n",
    "    compilation = spo.next(compilation)\n",
    "    albums.extend(compilation['items'])\n",
    "\n",
    "df = pd.DataFrame(data=albums,columns=['name','id','release_date','album_type'])\n",
    "df.to_excel(path+\"Hacken_Lee_Albums_Full.xlsx\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这将生成一个包含**4个**字段的完整专辑数据集：\n",
    "- `name` （专辑名称）\n",
    "- `id` （专辑的Spotify ID）\n",
    "- `release_date` （发行日期）\n",
    "- `album_type`  （专辑类型）\n",
    "\n",
    "![](img/ex0.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_overall = pd.read_excel(path+\"Hacken_Lee_Albums_Full.xlsx\")\n",
    "ex_overall.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 获取每张专辑的曲目数据\n",
    "针对每张专辑，使用`album_tracks()`方法获取专辑内所有曲目的信息。我们仅需提取每首曲目的`id`和`name`字段用于后续分析，同样使用`pandas`库进行数据处理。\n",
    "\n",
    "**注意事项**：在此步骤中必须加入`time.sleep()`延时操作，否则极易因请求频率过高导致进程中断。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(albums['id'])):\n",
    "    print(albums['name'][i],\" started\")\n",
    "   \n",
    "    results = spo.album_tracks(albums['id'][i])\n",
    "    tracks = results['items']\n",
    "    while results['next']:\n",
    "        results = spo.next(results)\n",
    "        tracks.extend(results['items'])\n",
    "        \n",
    "    tracks_df = pd.DataFrame(data=tracks,columns=['name','id'])\n",
    "    print(albums['name'][i],\" dataframe generated\")\n",
    "    tracks_df[\"album_id\"] = albums['id'][i]\n",
    "    tracks_df.to_excel(path+str(albums['name'][i])+\".xlsx\",index=False)\n",
    "    print(str(albums['name'][i]),\" finished\")\n",
    "    time.sleep(30) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至此，每张专辑均生成独立数据集，包含**3个**字段：\n",
    "- `name` （曲目名称）\n",
    "- `id` （曲目唯一标识）\n",
    "- `album_id` （所属专辑id）\n",
    "\n",
    "行数即为该专辑的曲目总量。示例如下：\n",
    "![](img/ex1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_album = pd.read_excel(path+\"李克勤慶祝成立30週年演唱會.xlsx\")\n",
    "ex_album.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 获取每首曲目的音频特征\n",
    "这是数据提取环节核心的一步。Spotify数据库最具价值的部分在于每首曲目的音频特征数据。`audio_features()`方法支持传入曲目`id`列表作为参数，这一设计显著优化了提取效率——通过批量获取音频特征（而非逐条查询），既能减少API调用次数，又可避免超时中断。\n",
    "\n",
    "继续使用`pandas`进行数据处理：剔除冗余列后保存结果文件。\n",
    "\n",
    "**注意事项**：\n",
    "1. 需谨慎设置`time.sleep()`参数值，建议根据API限制动态调整间隔时长\n",
    "2. `audio_features` 返回的数据不包含专辑/曲目名称，需手动关联\n",
    "    - `features_df['album_name'] = albums['name'][i]` *从总专辑数据集复制当前专辑名称（按索引定位）* \n",
    "    - `features_df.loc[:,'track_name'] = album['name']` *从当前数据集复制全部曲目名称* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(albums['id'])):\n",
    " \n",
    "    if (i>20 and i%20==0):\n",
    "        time.sleep(5)\n",
    "    print(albums['name'][i]+\" started\")\n",
    "   \n",
    "    album = pd.read_excel(path+albums['name'][i]+'.xlsx') \n",
    " \n",
    "    track_ids = []\n",
    "    for j in range(len(album['id'])):\n",
    "        track_ids.append(album['id'][j])\n",
    "    time.sleep(1)\n",
    "    print(\"Track ID concatenated\")\n",
    "    results = spo.audio_features(track_ids)\n",
    "    \n",
    "    features_df = pd.DataFrame(data=results,columns=results[0].keys())\n",
    "    print(\"DataFrame Generated\")\n",
    "    time.sleep(1)\n",
    "    features_df = features_df.drop(columns=['type','uri','track_href','analysis_url'])\n",
    " \n",
    "    features_df['album_name'] = albums['name'][i]\n",
    "    features_df.loc[:,'track_name'] = album['name']\n",
    "    features_df.to_excel(path+albums['name'][i]+'_features.xlsx',index=False)\n",
    "    \n",
    "    print(albums['name'][i]+\" finished\")\n",
    "    print('time to sleep')\n",
    "    time.sleep(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与上一步(2.3)类似，这将为每张专辑独立生成一个包含**14个**字段的数据集。示例如下：\n",
    "![](img/ex2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_album_feature = pd.read_excel(path+\"李克勤慶祝成立30週年演唱會_features.xlsx\")\n",
    "ex_album_feature.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 合并特征数据集为统一数据集\n",
    "为了便于快速检索和提取特定曲目信息，将所有特征数据集合并为一个总数居集将大幅提升效率。 \n",
    "\n",
    "**注意事项**：\n",
    "为了提高可读性，通过一下代码调整列顺序，确保`track_name`(曲目名称) `id`(曲目ID) `album_name`(专辑名称) 作为前三列显示：\n",
    "\n",
    "```python\n",
    "album = album.iloc[:, [15, 11, 14, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13]] \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "album_features = []\n",
    "total_albums_features = []  \n",
    "for i in range(len(albums['id'])):\n",
    "    album = pd.read_excel(path + albums['name'][i] + '_features.xlsx')\n",
    "    album = album.iloc[:, [15, 11, 14, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13]] \n",
    "    albums.append(album)\n",
    "\n",
    "album_features_total = pd.concat(total_albums_features)\n",
    "album_features_total.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 通过公共API获取播放量数据\n",
    "**免责声明**: 本项目仅用于**教育研究目的**。\n",
    "\n",
    "回到本项目的首个问题：*歌单中热门金曲与沧海遗珠的平衡程度*。要解答这个问题，首先需要明确定义何为*热门金曲*，何为*沧海遗珠*。每位歌迷可能有不同见解，因此必须引入客观量化标准。\n",
    "\n",
    "#### 2.6.1 Spotify流行度(`popularity`)数据的局限性\n",
    "Spotify API虽有提供曲目流行度数据（*“根据算法计算，主要基于总播放量及近期播放频率”*），但其机制导致同一曲目的不同版本会独立计算流行度，这在本项目中会产生偏差：\n",
    "- 合集专辑干扰：唱片公司常发行精选合辑，导致同一首歌的播放量分散在不同专辑版本中\n",
    "- 数据碎片化：听众可能通过精选辑播放经典曲目，而非原始专辑版本\n",
    "#### 2.6.2 采用公开API获取播放总量数据\n",
    "Spotify的APP/网页上也会显示曲目的播放量，尽管官方API未开放此数据。经过一番调查发现，Spotify实际会汇总同一曲目所有版本的播放量——这完美契合本项目需求。\n",
    "\n",
    "诚挚感谢[此github项目](https://github.com/entriphy/sp-playcount-librespot)的作者提供的可获取专辑内所有曲目的播放量的API。\n",
    "\n",
    "使用`requests`库处理API响应，以及通过`pandas`将播放量与现有特征数据集合并。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hacken_albums = pd.read_excel(path+\"Hacken_Lee_Albums_Full.xlsx\")\n",
    "\n",
    "base_url = \"https://api.t4ils.dev/albumPlayCount\"\n",
    "for i in range(len(hacken_albums['id'])):\n",
    "    if (i>61 and i%10==0):\n",
    "        print(\"it's time to sleep\")\n",
    "        time.sleep(5)\n",
    "    album_id = hacken_albums['id'][i]  \n",
    "\n",
    "  \n",
    "    url = f\"{base_url}?albumid={album_id}\"\n",
    "    response = requests.get(url)\n",
    "    print(hacken_albums['name'][i],\" started\")\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "    \n",
    "        if data.get('success'):\n",
    "            play_count_data = []\n",
    "            album_data = data.get('data',{})\n",
    "            for disc in album_data.get('discs', []):\n",
    "                for track in disc.get('tracks', []):\n",
    "                    track_id = track.get('uri').split(\":\")[-1] \n",
    "                    play_count = track.get('playcount')\n",
    "                    play_count_data.append({'id': track_id, 'play_count': play_count})\n",
    "            \n",
    "         \n",
    "            play_count_df = pd.DataFrame(play_count_data)\n",
    "            print(play_count_df.head())\n",
    "          \n",
    "            existing_data = pd.read_excel(path+hacken_albums['name'][i]+\"_features.xlsx\")\n",
    "            updated_data = existing_data.merge(play_count_df, on='id', how='left')\n",
    "        \n",
    "            updated_data.to_excel(path+hacken_albums['name'][i]+\"_features.xlsx\", index=False)\n",
    "            print(\"Play count data successfully merged into the Excel sheet.\")\n",
    "        \n",
    "        else:\n",
    "            print(\"API response indicates failure.\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data. HTTP Status Code: {response.status_code}\")\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6.3 Update the overall feature dataset 更新特征数据总集\n",
    "After grabbing the play count data for each album, the overall dataset needs to be updated as follows. \n",
    "在获取完每张专辑的播放量数据后，需要更新特征数据总集。\n",
    "\n",
    "**注意事项**：\n",
    "在特征数据总集里，专辑排列的顺序跟在专辑数据总集保持一致。通过利用这一特点，嵌套for循环即可简化更新流程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playcount_list = []\n",
    "\n",
    "for i in range(len(hacken_albums['id'])):\n",
    "    album_name = hacken_albums.loc[i, \"name\"]\n",
    "    print(f\"{album_name} started\")\n",
    "\n",
    "    album = pd.read_excel(path + album_name + \"_features.xlsx\")\n",
    "  \n",
    "    for j in range(len(album['id'])):\n",
    "        playcount_list.append(album.loc[j, 'play_count'])\n",
    "\n",
    "hacken_features = pd.read_excel(path + \"Hacken_Lee_Albums_Features_Full.xlsx\")\n",
    "hacken_features['play_count'] = playcount_list\n",
    "\n",
    "hacken_features.to_excel(path+\"Hacken_Lee_Albums_Features_Updated.xlsx\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Till now, everything needed from Spotify has been retrieved and saved in several datasets. Let's proceed to extract setlist data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extracting Setlist Data\n",
    "NOTE: In this part, only two overall datasets are needed:\n",
    "- `hacken_albums`: the overall album list which contains every album's `name`, `id`, `release_date` and `album_type`\n",
    "- `hacken_features`: the overall feature list which contains every track's audio features data, album_name, track_name, and play_count\n",
    "\n",
    "### 3.1 Searching for specific track data\n",
    "The first step is obviously to make a list of names of the tracks that Hacken performed in those three concerts respectively. Using `pandas`'s function `.isin()` can do a quick and shallow filter based on the tracks' name. But a problem emerges as follows: how to deal with duplicates? Pick my favourite song as an example, here is the result for `hacken_features[hacken_features['track_name'].str.contains(\"好戲之人\")][[\"track_name\",\"album_name\",\"play_count\"]]`.\n",
    "![](img/ex3.png)\n",
    "\n",
    "It's clearly that one single track would have several versions (studio album, compilation, or live). In order to answer the second research question: *Which albums are most favoured?*, only one version is needed, that is, the first album that the track is released in. \n",
    "\n",
    "NOTE1: Since the feature dataset doesn't contain information of release date, a merge operation is required. The logic of this merge step is to add the `release_date` cell value based on the match for album name. As the representative column of album name have different names (`album_name` in feature dataset and `name` in album dataset), an *inner join* is the only method that works.\n",
    "\n",
    "NOTE2: The type of `release_date` is string, it needs to be reformated to datetime object first. \n",
    "\n",
    "Below is an example code for extracting the data for setlist in Macau concert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_list_macau = [\n",
    "    \"希望\", \"一個都不能少\", \"破曉時份\", \"只想你會意\", \"舊歡如夢\", \n",
    "    \"護花使者\", \"藍月亮\", \"C3PO\", \"回首\", \"一生不變\", \n",
    "    \"Victory\", \"天水、圍城\", \"刻不容緩\", \"最愛\", \"沒有你贏了世界又如何\",\"告別校園時\",\n",
    "    \"再見演奏廳\",\"深深深\",\"紅日\",\"夏日之神話\",\"月半小夜曲\",\"我不會唱歌\",\"高妹\",\"合久必分\",\"飛花\"\n",
    "]\n",
    "\n",
    "\n",
    "filtered_df = hacken_features[hacken_features['track_name'].isin(track_list_macau)]\n",
    "\n",
    "\n",
    "merged_filtered_df = pd.merge(\n",
    "    filtered_df, \n",
    "    hacken_albums[['name', 'release_date']], \n",
    "    left_on='album_name', \n",
    "    right_on='name', \n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "\n",
    "merged_filtered_df['release_date'] = pd.to_datetime(merged_filtered_df['release_date'])\n",
    "\n",
    "plot_data = merged_filtered_df.loc[merged_filtered_df.groupby('track_name')['release_date'].idxmin()]\n",
    "\n",
    "plot_data['release_year'] = plot_data['release_date'].dt.year\n",
    "\n",
    "plot_data = plot_data.drop(columns=['release_date',\"name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Manual Adjustment\n",
    "#### 3.2.1 Correction for album name\n",
    "\n",
    "Due to copyright issues, some of Hacken's albums are not available on Spotify. (Particularly those released by Impact Entertainment.) Thus, in the last step of selecting the first release album, what is selected is the earliest album that Spotify has data for. To pursue authenticity, manual correction for albums is necessary in this step.\n",
    "\n",
    "#### 3.2.2 Adding record company information\n",
    "\n",
    "For visualization, I find it would be more intuitive if every track/album is labelled with a color corresponding to the record company.\n",
    "\n",
    "As said in the introduction, Hacken's career spans from the 1980s to 2020s (and is still ongoing), *unavoidably* (I use this word in considering the fact of record industry development in Hong Kong from the 1980s to now) he has collaborated with 5 different record companies:\n",
    "- PolyGram (*1986-1993*)\n",
    "- Star (*1993-1996*)\n",
    "- Music Impact Entertainment (*1996-1998*)\n",
    "- Universal Music Hong Kong (*1999-2016*)\n",
    "- Emperor Entertainment Group (*2016-now*)\n",
    "\n",
    "Although Spotify contains a copyright statement for an album, for some albums the information is missing. In this case, it would be more time-saving to enter the record company value manually as I have prior knowledge (with the help of those physical CDs I owned).\n",
    "\n",
    "#### 3.2.3 Classifying tracks based on position\n",
    "Normally a setlist of a concert is consisted of fixed songs and encore songs. Classifying the tracks into `main` and `encore` types allows me to have a better examine at the first research question.\n",
    "\n",
    "NOTE: Usually the encore part lies the last of a concert, but here the setlist of the Hong Kong one makes the exception -- as Hacken performed different songs every night during the orchestra's intermission time. Hence I feel more appropriate to put those four songs into the category of '*encore songs*'.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_name</th>\n",
       "      <th>id</th>\n",
       "      <th>album_name</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>play_count</th>\n",
       "      <th>release_year</th>\n",
       "      <th>record_company</th>\n",
       "      <th>rundown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C3PO</td>\n",
       "      <td>18BLNafwAjUQhmWXx3BY7E</td>\n",
       "      <td>30克</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.470</td>\n",
       "      <td>7</td>\n",
       "      <td>-8.663</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0418</td>\n",
       "      <td>0.734</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0963</td>\n",
       "      <td>0.373</td>\n",
       "      <td>70.122</td>\n",
       "      <td>232253</td>\n",
       "      <td>4</td>\n",
       "      <td>2303153</td>\n",
       "      <td>2017</td>\n",
       "      <td>英皇</td>\n",
       "      <td>main</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>一個都不能少</td>\n",
       "      <td>6vicNEMPZ2xyFMJwYGBRPz</td>\n",
       "      <td>30克</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.361</td>\n",
       "      <td>1</td>\n",
       "      <td>-9.536</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0328</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.1190</td>\n",
       "      <td>0.431</td>\n",
       "      <td>110.324</td>\n",
       "      <td>206360</td>\n",
       "      <td>4</td>\n",
       "      <td>890930</td>\n",
       "      <td>2017</td>\n",
       "      <td>英皇</td>\n",
       "      <td>main</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>一生不變</td>\n",
       "      <td>0I1bQXS2ktUPw1ANBw6vOX</td>\n",
       "      <td>Purple Dream</td>\n",
       "      <td>0.539</td>\n",
       "      <td>0.268</td>\n",
       "      <td>8</td>\n",
       "      <td>-18.405</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.0909</td>\n",
       "      <td>0.316</td>\n",
       "      <td>66.502</td>\n",
       "      <td>262373</td>\n",
       "      <td>4</td>\n",
       "      <td>5515647</td>\n",
       "      <td>1989</td>\n",
       "      <td>寶麗金</td>\n",
       "      <td>encore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>再見演奏廳</td>\n",
       "      <td>0Cv07c5pmSqhX1AV5LraTv</td>\n",
       "      <td>Hacken Lee No. 1 Hits</td>\n",
       "      <td>0.468</td>\n",
       "      <td>0.452</td>\n",
       "      <td>6</td>\n",
       "      <td>-9.802</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0402</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.0710</td>\n",
       "      <td>0.190</td>\n",
       "      <td>81.871</td>\n",
       "      <td>215440</td>\n",
       "      <td>4</td>\n",
       "      <td>138894</td>\n",
       "      <td>2007</td>\n",
       "      <td>環球</td>\n",
       "      <td>main</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>合久必婚</td>\n",
       "      <td>0YXOc4ngvaxhB0IJRFNvTh</td>\n",
       "      <td>Custom Made</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.618</td>\n",
       "      <td>6</td>\n",
       "      <td>-7.543</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0341</td>\n",
       "      <td>0.594</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1280</td>\n",
       "      <td>0.439</td>\n",
       "      <td>111.477</td>\n",
       "      <td>192627</td>\n",
       "      <td>4</td>\n",
       "      <td>11101715</td>\n",
       "      <td>2003</td>\n",
       "      <td>環球</td>\n",
       "      <td>encore</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  track_name                      id             album_name  danceability  \\\n",
       "0       C3PO  18BLNafwAjUQhmWXx3BY7E                    30克         0.538   \n",
       "1     一個都不能少  6vicNEMPZ2xyFMJwYGBRPz                    30克         0.528   \n",
       "2       一生不變  0I1bQXS2ktUPw1ANBw6vOX           Purple Dream         0.539   \n",
       "3      再見演奏廳  0Cv07c5pmSqhX1AV5LraTv  Hacken Lee No. 1 Hits         0.468   \n",
       "4       合久必婚  0YXOc4ngvaxhB0IJRFNvTh            Custom Made         0.629   \n",
       "\n",
       "   energy  key  loudness  mode  speechiness  acousticness  instrumentalness  \\\n",
       "0   0.470    7    -8.663     1       0.0418         0.734          0.000000   \n",
       "1   0.361    1    -9.536     1       0.0328         0.825          0.000042   \n",
       "2   0.268    8   -18.405     1       0.0368         0.485          0.000046   \n",
       "3   0.452    6    -9.802     1       0.0402         0.790          0.000133   \n",
       "4   0.618    6    -7.543     1       0.0341         0.594          0.000000   \n",
       "\n",
       "   liveness  valence    tempo  duration_ms  time_signature  play_count  \\\n",
       "0    0.0963    0.373   70.122       232253               4     2303153   \n",
       "1    0.1190    0.431  110.324       206360               4      890930   \n",
       "2    0.0909    0.316   66.502       262373               4     5515647   \n",
       "3    0.0710    0.190   81.871       215440               4      138894   \n",
       "4    0.1280    0.439  111.477       192627               4    11101715   \n",
       "\n",
       "   release_year record_company rundown  \n",
       "0          2017             英皇    main  \n",
       "1          2017             英皇    main  \n",
       "2          1989            寶麗金  encore  \n",
       "3          2007             環球    main  \n",
       "4          2003             環球  encore  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hk_setlist = pd.read_excel(path+\"23HongKong.xlsx\")\n",
    "hk_setlist.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualization\n",
    "### 4.1 Quadrant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from adjustText import adjust_text\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "\n",
    "matplotlib.rcParams['font.family'] = ['Heiti TC']\n",
    "\n",
    "no_duplicate_df = pd.read_excel(path+\"23HLnoduplicate.xlsx\")\n",
    "\n",
    "\n",
    "no_duplicate_df['log_play_count'] = np.log1p(no_duplicate_df['play_count'])\n",
    "\n",
    "color_id = {\n",
    "     '寶麗金': '#84594d',  # Blue\n",
    "    '星光': '#a9d8e4',  # Orange\n",
    "    '藝能': '#c6c6c6',  # Green\n",
    "    '環球': '#a7dc91',  # Red\n",
    "    '英皇': '#3975b1'   # Purple\n",
    "}\n",
    "tran_Name = {\n",
    "    '寶麗金': 'PolyGram', \n",
    "    '星光': 'Star',  \n",
    "    '藝能': 'Music Impact Entertainment',\n",
    "    '環球': 'Universal', \n",
    "    '英皇': 'EEG'  \n",
    "}\n",
    "\n",
    "\n",
    "no_duplicate_df['RecordCompanyEN'] = no_duplicate_df['record_company'].map(tran_Name)\n",
    "no_duplicate_df['color'] = no_duplicate_df['record_company'].map(color_id)\n",
    "rundown_color_map = {'main': 'black', 'encore': '#c9a1ed'}\n",
    "no_duplicate_df['text_color'] = no_duplicate_df['rundown'].map(rundown_color_map)\n",
    "\n",
    "x = no_duplicate_df['release_year']\n",
    "y = no_duplicate_df['log_play_count']\n",
    "\n",
    "\n",
    "x_min, x_max = 1985, 2025\n",
    "y_min, y_max = 8,18\n",
    "x_mid = x_min + (x_max - x_min) / 2\n",
    "y_mid = y_min + (y_max - y_min) / 2\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "for company, group in no_duplicate_df.groupby('RecordCompanyEN'):\n",
    "    ax.scatter(\n",
    "        group['release_year'],\n",
    "        group['log_play_count'],\n",
    "        label=company,\n",
    "        color=group['color'].iloc[0],\n",
    "        edgecolor='black',\n",
    "        s=100\n",
    "        \n",
    "    )\n",
    "\n",
    "texts = []\n",
    "for i, row in no_duplicate_df.iterrows():\n",
    "    texts.append(\n",
    "        ax.text(\n",
    "            row['release_year']+0.3,\n",
    "            row['log_play_count']-0.1,\n",
    "            row['track_name'],\n",
    "            fontsize=12,\n",
    "            ha='left',\n",
    "            color=row['text_color']\n",
    "        )\n",
    "    )\n",
    "adjust_text(texts,arrowprops=dict(arrowstyle='->', color='red'))\n",
    "\n",
    "\n",
    "ax.axvline(x=x_mid, color='#c2d6f3', linestyle='--', linewidth=1)\n",
    "ax.axhline(y=y_mid, color='#c2d6f3', linestyle='--', linewidth=1)\n",
    "ax.set_xlim(1985,2025)\n",
    "ax.set_ylim(8,18)\n",
    "\n",
    "ax.set_title('Hacken Lee X HKPhil Concert 2023 Rundown', fontsize=16)\n",
    "ax.set_xlabel('Release Year', fontsize=12)\n",
    "ax.set_ylabel('Log-Scaled Popularity', fontsize=12)\n",
    "ax.legend(title=\"Record Company\", loc='upper right')\n",
    "plt.text(0.85,\n",
    "         0.77,\n",
    "         \"a watermark by wend1k3\",\n",
    "         transform=plt.gca().transAxes,\n",
    "            ha='center',  \n",
    "            va='center',  \n",
    "            alpha=1,\n",
    "                fontdict=dict(\n",
    "                    fontsize=11,\n",
    "                    color='#ecacbd',\n",
    "                    family=\n",
    "                    \"Heiti TC\",  \n",
    "                    weight=\n",
    "                    'normal', \n",
    "                )  )\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above example code will yield a four-quadrant visualization where x-axis represents the track's release year and y-axis represents the popularity of the track. \n",
    "![](img/23HKquadrant.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Bar plot "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Discussion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
